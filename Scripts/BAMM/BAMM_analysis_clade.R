rm(list=ls())
library("BAMMtools") 
library("coda")

# assuming event data and mcmc out files generated by BAMM
# are store under folder named by each of 17 orders
# e.g. "working_dir/Rosales/Rosales_event_data_final.txt"
# e.g. "working_dir/Rosales/Rosales_mcmc_out_final.txt"

cladelist <- read.csv("Order", header=F)

Order <- as.character(cladelist$V1)

#cat("clade,logLik,N_shift\n", file="Rosids_17Order_PB_ESS_convergence.txt")

#BAMMresult

tryCatch({
	for(i in 1:17){
	  
	  clade <- Order[i]
	  
	  tree <- read.tree(paste0("./data/", clade, ".tre", sep=""))
	  
	  #BAMM object
	  edata <- getEventData(tree, paste0("./data/", clade, "_PB_eventdata_cmb.csv", sep=""), burnin = 0.3)
	  #summary(edata)
	  saveRDS(edata,file=paste0("./BAMMresult/", clade, "_PB_edata.rds", sep=""))
	  #edata <- readRDS(file=paste("./BAMMresult/", clade, "_PB_edata.rds", sep=""))
	  
	  #Assessing MCMC convergence
	  mcmc <- read.csv(paste0("./data/", clade, "_PB_mcmc_out_cmb.csv", sep=""), header=T)
	  pdf(paste("./BAMMresult/", clade, "_PB_MCMC_convergent.pdf", sep=""))
	  plot(mcmc$logLik ~ mcmc$generation)
	  dev.off()
	  
	  burnstart <- floor(0.3*nrow(mcmc))
	  postburn <- mcmc[burnstart:nrow(mcmc), ]#postburn is the generations left after burning
	  
	  #next caculate the effective sample size (ESS), which should be greater than 200 if our analysis
	  # ran long enough
	  logLik <- effectiveSize(postburn$logLik) # calculates autocorrelation function
	  N_shift <- effectiveSize(postburn$N_shift) #effective sample size on N-shifts
	  ESSample <- cbind.data.frame(clade, logLik, N_shift)
	  
	  write.table(ESSample, "./BAMMresult/Rosids_17Order_PB_ESS_convergence.txt", col.names=F, row.names=F, quote=F, append=T, sep=",")
	  
	  #tip rates
	  TR <- getTipRates(edata, returnNetDiv = FALSE, statistic = "median")
	  file <- paste0("./BAMMresult/", clade, "_BAMM_PB_TipRates.csv", sep="")
	  cat("Tip_label,Rate\n", file=file)
	  write.table(TR$lambda.avg, sep=",", file=file, col.names = FALSE, quote=F, append = TRUE)
	  
	  ##### Prepare Mean Rate Matrix ####
	  rtt <- getRateThroughTimeMatrix(edata)
	  Mean.lamda <- apply(rtt$lambda, 2, quantile,  c(0.5))
	  Mean.mu <- apply(rtt$mu, 2, quantile,  c(0.5))
	  mean.netdiv <- Mean.lamda - Mean.mu
	  
	  Mean.Rate.Matrix <- cbind.data.frame(rtt$times, Mean.lamda, Mean.mu, mean.netdiv)
	  write.csv(Mean.Rate.Matrix, paste0("./BAMMresult/", clade, "_PB_Mean_Rate_Matrix.csv", sep=""), row.names=F, quote=F)
	  
	  saveRDS(rtt, paste("./BAMMresult/", clade, "_PB_RateThroughTimeMatrix.rds", sep=""))
	}
})

